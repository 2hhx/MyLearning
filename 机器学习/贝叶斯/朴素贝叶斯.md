条件概率：
条件概率（又称后验概率）就是事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。

联合概率：P(A,B)

边缘概率(先验概率)：P(A)或者P(B)

$p(c | E)=\frac{p(E | c) p(c)}{p(E)}$

Naïve Bayes classifier is also easy to implement. The most time-consuming part is how to compute p(E | c) in Eq. 1. This probability calculation is important to make the classifier more accurate. In basic literatures of pattern recognition or machine learning, it is proposed that this probability can be estimated by some standard data distribution such as Gaussian or Poisson [20].
****
$P(Y | X)=\frac{P(X | Y) * P(Y)}{P(X)}$

我们可以把特征 X 当成是条件事件，而要求解的标签 Y 当成是满足条件后会被影响的结果，而两者之间的概率关系就是 
- $P(Y | X)$。这个概率在机器学习中，被称为是**标签的后验概率**(posterior probability), 即是说我们**先知道了条件，再去求解结果。** 
- 而标签 Y **在没有任何条件限制下**取值为某个值的概率，被写作$P(Y)$，与后验概率相反，这是**没有任何条件限制的**，**标签的先验概率**(prior probability)。
- 而$P(X|Y)$被称为“**类的条件概率**”，表示当 Y 的取值固定的时候，X 为某个值的概率。

# 朴素贝叶斯
机器学习中的简写 $P(Y)$，通常表示标签取少数类的概率，少数类往往用正样本表示，也就是$P(Y=1)$，本质就是所有样本中标签为1 的样本所占的比例。如果没有样本不均衡问题，则必须要在求解的时候明确，你的 Y 的取值到底是什么。
## 求解P(X∣Y=1)
**$P(X∣Y=1)$是对于任意一个样本而言的。** 对每一个样本进行计算，不是对整个特征矩阵，更不是对某一个标签。

$P(Y=1 | \boldsymbol{X})=\frac{P(\boldsymbol{X} | Y=1) * P(Y=1)}{P(\boldsymbol{X})} =\frac{P\left(x_{1}, x_{2} \ldots x_{n} | Y=1\right) * P(Y=1)}{P\left(x_{1}, x_{2} \ldots x_{n}\right)}$

$P(\boldsymbol{X} | Y=1)=\prod_{i=1}^{n} P\left(X_{i}=x_{i} | Y=1\right)$

**这个式子证明，在Y=1的条件下，多个特征的取值被同时取到的概率，就等于Y=1的条件下，多个特征的取值被分别取到的概率相乘**


**假设特征之间是有条件独立的，** 可以解决众多问题，也简化了很多计算过程，**这是朴素贝叶斯被称为”朴素“的理由**。因此，贝叶斯在特征之间有较多相关性的数据集上表现不佳，而现实中的数据多多少少都会有一些相关性，所以贝叶斯的分类效力在分类算法中不算特别强大。同时，一些影响特征本身的相关性的降维算法，比如PCA和SVD，和贝叶斯连用效果也会不佳。

## 求解P(X)
**可以使用全概率公式来求解 $P(X)$:**

$P(\boldsymbol{X})=\sum_{i=1}^{m} P\left(y_{i}\right) * P\left(\boldsymbol{X} | Y_{i}\right)$

二分类：
$P(\boldsymbol{X})=P(Y=1) * P(\boldsymbol{X} | Y=1)+P(Y=0) * P(\boldsymbol{X} | Y=0)$


## P(Y|X)
对于每一个样本，我们不可能只有一个特征X，而是会存在包含n个特征的取值的特征向量$\boldsymbol{X}$。因此机器学习中的后验概率，被写作P(Y|X)，其中
- $\boldsymbol{X}$代表特征向量中n个特征上的取值。
- 每个特征被写作$\boldsymbol{X_i}$, i 代表这个特征的编号，
- 每个特征在一个样本上的取值被写作$x_i$,代表在 i 这个特征下，样本的取值是$x_i$。
- 由此，粗体的$\boldsymbol{X}$可以表示为
$\boldsymbol{X} = \{X_1 = x_1,X_2 = x_2,...X_n = x_n\}$

## 不同写法
下面这些写法表达的都是一个意思

- **一个样本上**， **所有特征取值下**的概率：
每一个向量都是一个样本上的特征向量，其中$\boldsymbol{X}$和$\boldsymbol{x}$都是粗体，代表向量，可以展开。
$\begin{aligned}
&P(\boldsymbol{X})\\
&P(\boldsymbol{x})\\
&P\left(X_{1}=x_{1}, X_{2}=x_{2}, \dots, X_{n}=x_{n}\right)\\
&P\left(X_{1}, X_{2}, \dots, X_{n}\right)\\
&P\left(x_{1}, x_{2}, \dots, x_{n}\right)
\end{aligned}$

- **一个样本上**， **一个特征**所取值下的概率：是对于单独元素来说，没有向量存在。

  $\begin{aligned}
&P\left(X_{i}=x_{i}\right)\\
&P\left(X_{i}\right)\\
&P\left(x_{i}\right)
\end{aligned}$


# 高斯朴素贝叶斯
通过假设$P(x_i|Y)$是服从高斯分布，来估计每个特征下每个类别上的条件概率。对于每个特征下的取值，高斯朴素贝叶斯有如下公式：
$\begin{aligned}
P\left(x_{i} | Y\right) &=f\left(x_{i} ; \mu_{y}, \sigma_{y}\right) * \epsilon \\
&=\frac{1}{\sqrt{2 \pi \sigma_{y}^{2}}} \exp \left(-\frac{\left(x_{i}-\mu_{y}\right)^{2}}{2 \sigma_{y}^{2}}\right)
\end{aligned}$

$\epsilon$就是$f(x_i) * \epsilon$来近似表示面积