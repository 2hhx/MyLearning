条件概率：
条件概率（又称后验概率）就是事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。

联合概率：P(A,B)

边缘概率(先验概率)：P(A)或者P(B)
# 朴素贝叶斯概率模型
朴素贝叶斯分类器的一个优势在于只要根据少量的训练数据估计出必要的参数（变量的均值和方差）
$P(C | F1,...Fn)=\frac{P(C)P(F1,...Fn | C)}{P(F1,...Fn)}$

用朴素的语言可以表达为：
$$
posterior = \frac{prior \times likelihood}{evidence}
$$
不考虑evidence的话，这个式子的抽象含义是：对于给定观测数据，一个猜测是好是坏，取决于 **“这个猜测本身独立的可能性大小（先验概率，Prior ）”和“这个猜测生成我们观测到的数据的可能性大小”（似然，Likelihood ）的乘积。**

实际中，我们只关心分式中的分子部分，因为分母不依赖于C而且特征 $F_i$ 的值是给定的，于是分母可以认为是一个常数。这样分子就等价于联合分布模型。
$$
p(C,F1,...Fn)
$$ 
重复使用链式法则，可以将该式写成条件概率的形式，如下所示：
$$
\begin{array}{l}
{p\left(C, F_{1}, \ldots, F_{n}\right)} \\
{\quad \propto p(C) p\left(F_{1}, \ldots, F_{n} | C\right)} \\
{\quad \propto p(C) p\left(F_{1} | C\right) p\left(F_{2}, \ldots, F_{n} | C, F_{1}\right)} \\
{\quad \propto p(C) p\left(F_{1} | C\right) p\left(F_{2} | C, F_{1}\right) p\left(F_{3}, \ldots, F_{n} | C, F_{1}, F_{2}\right)} \\
{\quad \propto p(C) p\left(F_{1} | C\right) p\left(F_{2} | C, F_{1}\right) p\left(F_{3} | C, F_{1}, F_{2}\right) p\left(F_{4}, \ldots, F_{n} | C, F_{1}, F_{2}, F_{3}\right)} \\
{\quad \propto p(C) p\left(F_{1} | C\right) p\left(F_{2} | C, F_{1}\right) p\left(F_{3} | C, F_{1}, F_{2}\right) \ldots p\left(F_{n} | C, F_{1}, F_{2}, F_{3}, \ldots, F_{n-1}\right)}
\end{array}
$$
现在 “朴素” 的条件独立假设开始发挥作用：假设每个特征$F_i$对于其他特征$F_j,i \neq j$是条件独立的。这就意味着

$p(F_i|C,F_j) = p(F_i|C)$

对于$i \neq j$，所以联合分布模型可以表示为
$$
\begin{aligned}
p\left(C | F_{1}, \ldots, F_{n}\right) & \propto p\left(C, F_{1}, \ldots, F_{n}\right) \\
& \propto p(C) p\left(F_{1} | C\right) p\left(F_{2} | C\right) p\left(F_{3} | C\right) \cdots \\
& \propto p(C) \prod_{i=1}^{n} p\left(F_{i} | C\right)
\end{aligned}
$$
这意味着上述假设下，类变量$C$的条件分布可以表达为：
$$
p\left(C | F_{1}, \ldots, F_{n}\right)=\frac{1}{Z} p(C) \prod_{i=1}^{n} p\left(F_{i} | C\right)
$$
## 从概率模型中构造分类器：
朴素贝叶斯分类器包括了这种模型和相应的决策规则。一个普通的规则就是选出最有可能的那个：这就是大家熟知的最大后验概率（MAP）决策准则。相应的分类器便是如下定义的$classify$公式：
$$
\text { classify }\left(f_{1}, \ldots, f_{n}\right)=\underset{c}{\operatorname{argmax}} p(C=c) \prod_{i=1}^{n} p\left(F_{i}=f_{i} | C=c\right)
$$
# 参数估计
所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计。类的先验概率可以通过假设各类等概率来计算（先验概率 = 1 / (类的数量)），或者通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）/(样本总数)）。为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。
## 高斯朴素贝叶斯
**如果要处理的是连续数据一种通常的假设是这些连续数值为高斯分布。** 例如，假设训练集中有一个连续属性，$x$。我们首先对数据根据类别分类，然后计算每个类别中$x$的均值和方差。令$\mu _{c}$ 表示为$x$在$c$类上的均值，令$\sigma _{c}^{2}$为 $x$在$c$类上的方差。在给定类中某个值的概率，$P(x=v|c)$，可以通过将$v$表示为均值为$\mu _{c}$方差为$\sigma _{c}^{2}$正态分布计算出来。如下：
$$
P(x=v | c)=\frac{1}{\sqrt{2 \pi \sigma_{c}^{2}}} e^{-\frac{\left(v-\mu_{c}\right)^{2}}{2 \sigma_{c}^{2}}}
$$
**处理连续数值问题的另一种常用的技术是通过离散化连续数值的方法。**

通常，当训练样本数量较少或者是精确的分布已知时，通过概率分布的方法是一种更好的选择。在大量样本的情形下离散化的方法表现更优，因为大量的样本可以学习到数据的分布。由于朴素贝叶斯是一种典型的用到大量样本的方法（越大计算量的模型可以产生越高的分类精确度），所以朴素贝叶斯方法都用到离散化方法，而不是概率分布估计的方法。
****

$p(c | E)=\frac{p(E | c) p(c)}{p(E)}$

Naïve Bayes classifier is also easy to implement. The most time-consuming part is how to compute p(E | c) in Eq. 1. This probability calculation is important to make the classifier more accurate. In basic literatures of pattern recognition or machine learning, it is proposed that this probability can be estimated by some standard data distribution such as Gaussian or Poisson [20].
****

$P(Y | X)=\frac{P(X | Y) * P(Y)}{P(X)}$

我们可以把特征 X 当成是条件事件，而要求解的标签 Y 当成是满足条件后会被影响的结果，而两者之间的概率关系就是 
- $P(Y | X)$。这个概率在机器学习中，被称为是**标签的后验概率**(posterior probability), 即是说我们**先知道了条件，再去求解结果。** 
- 而标签 Y **在没有任何条件限制下**取值为某个值的概率，被写作$P(Y)$，与后验概率相反，这是**没有任何条件限制的**，**标签的先验概率**(prior probability)。
- 而$P(X|Y)$被称为“**类的条件概率**”，表示当 Y 的取值固定的时候，X 为某个值的概率。

# 朴素贝叶斯
机器学习中的简写 $P(Y)$，通常表示标签取少数类的概率，少数类往往用正样本表示，也就是$P(Y=1)$，本质就是所有样本中标签为1 的样本所占的比例。如果没有样本不均衡问题，则必须要在求解的时候明确，你的 Y 的取值到底是什么。
## 求解P(X∣Y=1)
**$P(X∣Y=1)$是对于任意一个样本而言的。** 对每一个样本进行计算，不是对整个特征矩阵，更不是对某一个标签。

$P(Y=1 | \boldsymbol{X})=\frac{P(\boldsymbol{X} | Y=1) * P(Y=1)}{P(\boldsymbol{X})} =\frac{P\left(x_{1}, x_{2} \ldots x_{n} | Y=1\right) * P(Y=1)}{P\left(x_{1}, x_{2} \ldots x_{n}\right)}$

$P(\boldsymbol{X} | Y=1)=\prod_{i=1}^{n} P\left(X_{i}=x_{i} | Y=1\right)$

**这个式子证明，在Y=1的条件下，多个特征的取值被同时取到的概率，就等于Y=1的条件下，多个特征的取值被分别取到的概率相乘**


**假设特征之间是有条件独立的，** 可以解决众多问题，也简化了很多计算过程，**这是朴素贝叶斯被称为”朴素“的理由**。因此，贝叶斯在特征之间有较多相关性的数据集上表现不佳，而现实中的数据多多少少都会有一些相关性，所以贝叶斯的分类效力在分类算法中不算特别强大。同时，一些影响特征本身的相关性的降维算法，比如PCA和SVD，和贝叶斯连用效果也会不佳。

## 求解P(X)
**可以使用全概率公式来求解 $P(X)$:**

$P(\boldsymbol{X})=\sum_{i=1}^{m} P\left(y_{i}\right) * P\left(\boldsymbol{X} | Y_{i}\right)$

二分类：
$P(\boldsymbol{X})=P(Y=1) * P(\boldsymbol{X} | Y=1)+P(Y=0) * P(\boldsymbol{X} | Y=0)$


## P(Y|X)
对于每一个样本，我们不可能只有一个特征X，而是会存在包含n个特征的取值的特征向量$\boldsymbol{X}$。因此机器学习中的后验概率，被写作P(Y|X)，其中
- $\boldsymbol{X}$代表特征向量中n个特征上的取值。
- 每个特征被写作$\boldsymbol{X_i}$, i 代表这个特征的编号，
- 每个特征在一个样本上的取值被写作$x_i$,代表在 i 这个特征下，样本的取值是$x_i$。
- 由此，粗体的$\boldsymbol{X}$可以表示为
$\boldsymbol{X} = \{X_1 = x_1,X_2 = x_2,...X_n = x_n\}$

## 不同写法
下面这些写法表达的都是一个意思

- **一个样本上**， **所有特征取值下**的概率：
每一个向量都是一个样本上的特征向量，其中$\boldsymbol{X}$和$\boldsymbol{x}$都是粗体，代表向量，可以展开。
$\begin{aligned}
&P(\boldsymbol{X})\\
&P(\boldsymbol{x})\\
&P\left(X_{1}=x_{1}, X_{2}=x_{2}, \dots, X_{n}=x_{n}\right)\\
&P\left(X_{1}, X_{2}, \dots, X_{n}\right)\\
&P\left(x_{1}, x_{2}, \dots, x_{n}\right)
\end{aligned}$

- **一个样本上**， **一个特征**所取值下的概率：是对于单独元素来说，没有向量存在。

  $\begin{aligned}
&P\left(X_{i}=x_{i}\right)\\
&P\left(X_{i}\right)\\
&P\left(x_{i}\right)
\end{aligned}$


# 高斯朴素贝叶斯
通过假设$P(x_i|Y)$是服从高斯分布，来估计每个特征下每个类别上的条件概率。对于每个特征下的取值，高斯朴素贝叶斯有如下公式：
$\begin{aligned}
P\left(x_{i} | Y\right) &=f\left(x_{i} ; \mu_{y}, \sigma_{y}\right) * \epsilon \\
&=\frac{1}{\sqrt{2 \pi \sigma_{y}^{2}}} \exp \left(-\frac{\left(x_{i}-\mu_{y}\right)^{2}}{2 \sigma_{y}^{2}}\right)
\end{aligned}$

$\epsilon$就是$f(x_i) * \epsilon$来近似表示面积

# 最大似然和贝叶斯
估计类别的时候
$P(h | D) ∝ P(h) * P(D | h)$ 这里的h相当于我们的c（模型，参数）,D相当于上文的F（观测值）
- 贝叶斯派认为需要用最大后验概率：计算$P(h) * P(D | h)$，取最大的。贝叶斯派认为最好的估计需要同时考虑该模型生成当前特征值的可能性和模型本身先验概率（出现频率）
- 频率派的最大似然估计只考虑 P(D | h)，也就是寻找最可能产生当前观测值的模型而不考虑该模型本身的先验概率，当然对于先验概率均等的情况则两者一致。如果对先验概率一无所知，则也只能采取最大似然估计。