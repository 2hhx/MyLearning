# 联合概率
- $P(a,b) = P(a|b)P(b)$
- $P(a,b,c)= P(a|b,c)P(b|c)P(c)$

# 独立
$P(A|B) = P(A) if A \perp B$
- 因为$P(A | B)=\frac{P(A, B)}{P(B)}$永远成立
- 所以如果A，B是独立的那么
- $P(A,B) = P(A|B) P(B)$
  - 又因为$P(A|B) = P(A)$,所以
  - $P(A,B) = P(A)P(B)$

# 联合概率 P(A,B)
- P(A,B)=P(A∣B)P(B)：always true
- 当A，B独立同分布时，P(A,B) = P(A)P(B)

# 边缘概率 P(A)、P(B)
有时候，我们知道了一组变量的联合概率分布，但想要了解**其中一个子集的概率分布**。这种定义在子集上的概率分布被称为 **边缘概率分布**(marginal probability distribution)。

为什么叫**边缘概率分布**呢？ 举个例子，假如有两个随机变量x, y的二维联合概率分布如下表：
![](https://images2018.cnblogs.com/blog/1258764/201808/1258764-20180815114235758-629263311.png)

其中P(x)和P(y)为分别考虑随机变量x和y时的概率，我们通常将其**写在表格的边缘处**，故称之为边缘概率。

从上表可以看出，P(x=0)的概率为x=0时对y的各概率相加，也就是0+1/5=1/5，用公式表示为：
$$\forall x \in \mathrm{x}, P(\mathrm{x}=x)=\sum_{y} P(\mathrm{x}=x, \mathrm{y}=y)$$
这称为**求和法则（sum rule）**。

若随机变量为**连续型**的，我们需要用**积分代替求和**：
  $$
  p(x)=\int p(x, y) d y
  $$

# 变量消除
**求边缘分布的方法是消除其余的变量**，如何消除呢？如上例子，求 XX 的边缘分布就要对**联合分布在 YY上 加和（离散）或者积分（连续）**

对于很多个随机变量比如$U、V、W、X、Y、Z$，求X 的边缘分布的公式如下：
$$
\begin{aligned}
P(X)=& \sum_{U} \sum_{V} \sum_{W} \sum_{Y} \sum_{Z} P(U, V, W, X, Y, Z) \\
&=\sum_{U, V, W, Y, Z} P(U, V, W, X, Y, Z)
\end{aligned}
$$

# 条件概率
一个事件已经发生的情况下，另一个事件发生的概率，这种概率叫做条件概率。
$$
P(\mathrm{y}=y | \mathrm{x}=x)=\frac{P(\mathrm{y}=y, \mathrm{x}=x)}{P(\mathrm{x}=x)}
$$
# 条件概率的链式法则
$$P\left(\mathrm{x}^{(1)}, \ldots, \mathrm{x}^{(n)}\right)=P\left(\mathrm{x}^{(1)}\right) \Pi_{i=2}^{n} P\left(\mathrm{x}^{(i)} | \mathrm{x}^{(1)}, \ldots, \mathrm{x}^{(i-1)}\right)$$

# 用机器学习视角理解贝叶斯
$P(Y | X)=\frac{P(X | Y) * P(Y)}{P(X)}$
- 在机器学习的视角下，我们把X理解成“具有某特征”，把Y理解成“类别标签”
$$P(“属于某类” | “具有某特征”)=\frac{P(“具有某特征” | “属于某类”) * P(“属于某类”)}{P(“具有某特征”)}$$
- 贝叶斯方法把计算 “具有某特征的条件下属于某类” 的概率转换成需要计算 “属于某类的条件下具有某特征” 的概率，属于有监督学习。

# 垃圾邮件例子
给你一句话 “我司可办理正规发票（保真）17%增值税发票点数优惠”
1. 分词之后的贝叶斯
$$
\begin{aligned}
  &P("垃圾邮件"|（“我”，“司”，“办理”，“正规发票”，“保真”，“增值税”，“发票”，“点数”，“优惠”）)\\
  &=\frac{P(（“我”，“司”，“办理”，“正规发票”，“保真”，“增值税”，“发票”，“点数”，“优惠”）|"垃圾邮件")P("垃圾邮件")}{P(（“我”，“司”，“办理”，“正规发票”，“保真”，“增值税”，“发票”，“点数”，“优惠”）)}
\end{aligned}
$$

2.条件独立假设
$$
\begin{aligned}
& P(（“我”，“司”，“办理”，“正规发票”，“保真”，“增值税”，“发票”，“点数”，“优惠”）|"垃圾邮件")P("垃圾邮件")\\
&=P(“我”|"垃圾邮件") \times P(“司”|"垃圾邮件") \times P(“办理”|"垃圾邮件") \times P(“正规发票”|"垃圾邮件") \times P(“保真”|"垃圾邮件") \times P(“增值税”|"垃圾邮件") \times P(“发票”|"垃圾邮件") \times P(“点数”|"垃圾邮件") \times P(“优惠”|"垃圾邮件") 
\end{aligned}
$$

3. 比较后验概率大小
$$
\begin{aligned}
& C=P(“我”|"垃圾邮件") \times P(“司”|"垃圾邮件") \times P(“办理”|"垃圾邮件") \times P(“正规发票”|"垃圾邮件") \times P(“保真”|"垃圾邮件") \times P(“增值税”|"垃圾邮件") \times P(“发票”|"垃圾邮件") \times P(“点数”|"垃圾邮件") \times P(“优惠”|"垃圾邮件")P(垃圾邮件) \\
& \bar{C} =P(“我”|"正常邮件") \times P(“司”|"正常邮件") \times P(“办理”|"正常邮件") \times P(“正规发票”|"正常邮件") \times P(“保真”|"正常邮件") \times P(“增值税”|"正常邮件") \times P(“发票”|"正常邮件") \times P(“点数”|"正常邮件") \times P(“优惠”|"正常邮件") P(正常邮件) \\
\end{aligned}
$$
- 每一项都特别好求：
  - 只需要分别统计各类邮件中该关键字出现的概率就可以了。比如
  - $P("发票"|S) = \frac{垃圾邮件中所有“发票”的次数}{垃圾邮件中所有词语的次数}$

4. 平滑技术
拉普拉斯加一：对于那些没有出现过的词语，如果不处理的话概率就变成0了。

- 比如“正规发票”没出现：
$$
P("正规发票"|S) = \frac{出现“正规发票”的垃圾邮件的封数+1}{每封垃圾邮件中所有次出现次数(出现了只计算一次)的总和+2}
$$
- 既然平白无故增加了没出现过的词的概率，对于那些出现过的词的词频就要降低。也采用拉普拉斯+1，但是分母就不是+2了，要加多一点。


# 最大似然估计（MLE）和 最大后验概率估计（MAP）
最大似然估计（Maximum likelihood estimation, 简称MLE）和最大后验概率估计（Maximum a posteriori estimation, 简称MAP）是很常用的两种参数估计方法。
## 概率和统计
**概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。**

 